# @package _global_

# to execute this experiment run:
# python train.py experiment=local-1

defaults:
  - override /model: mfa


# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345

model:
  corpus_dir: /datasets/LibriLight/
  manifests_dir: data/LibriHeavy
  
  mfa_tokenizer:
    textgrid_dir: /datasets/LibriLight_aligned/textgrids
  
  duration_predictor:
    depth: 8  # for english

  cfm_wrapper:
    cond_drop_prob: 0.2
    
  optim:
    name: adam
    lr: 1e-4

    sched:
      name: WarmupAnnealing
      warmup_steps: 5000
      max_steps: ${..lr}

  train_ds:
    max_duration: 16 # ~1600 frames of paper settings
    min_duration: 0.1
    num_workers: 8
    manifest_filepath: ${model.manifests_dir}/libriheavy_cuts_large.jsonl.gz
    lhotse:
      batch_duration: 300

  validation_ds:
    num_workers: 8
    lhotse:
      batch_duration: 300
  
  test_ds:
    num_workers: 8
    lhotse:
      batch_duration: 300

trainer:
  devices: auto
  max_epochs: null
  max_steps: 500000 # audio model steps
  
  gradient_clip_algorithm: norm
  gradient_clip_val: .2

exp_manager:
  # save_dir = ${exp_dir}/${name}/${version}
  exp_dir: null # defaults to ./nemo_experiments
  name: ${task_name}
  version: null # defaults to datetime

  resume_if_exists: False
  create_checkpoint_callback: True
  # checkpoint_callback_params:
  #   monitor: val_loss_total

# task name, determines output directory path
task_name: "ngc"

# set False to skip model training
train: True

# evaluate on test set, using best model weights achieved during training
# lightning chooses best weights based on the metric specified in checkpoint callback
test: False